# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Hk3nytblhxGQC8EljCTZIte0DEIs9ZP7
"""

!pip install umap

from googleapiclient.discovery import build
import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
import librosa
import librosa.display
import cv2
import numpy as np
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import umap

# API Key from Google Cloud Platform
API_KEY = 'AIzaSyAX0v9Ed6c-dchjSg9BQc-AwyPx3bDhzWQ'
YOUTUBE_API_SERVICE_NAME = 'youtube'
YOUTUBE_API_VERSION = 'v3'

nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

# Step 1: Fetch Video Data from YouTube API
def get_video_data(query, max_results):
    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION, developerKey=API_KEY)
    request = youtube.search().list(q=query, part='id,snippet', type='video', maxResults=max_results)
    response = request.execute()

    video_data = []
    for item in response['items']:
        video_id = item['id']['videoId']
        title = item['snippet']['title']
        description = item['snippet']['description']
        video_data.append((video_id, title, description))

    return pd.DataFrame(video_data, columns=['video_id', 'title', 'description'])

df_videos = get_video_data('educational videos', 10)

# Step 2: Text Preprocessing
def preprocess_text(text):
    text = text.lower()  # Convert to lowercase
    text = re.sub(r'\W', ' ', text)  # Remove punctuation
    text = re.sub(r'\s+', ' ', text)  # Remove extra spaces
    text = ' '.join([word for word in text.split() if word not in stop_words])  # Remove stopwords
    return text

df_videos['cleaned_description'] = df_videos['description'].apply(preprocess_text)

# Step 3: Extract Text Features
def extract_text_features(text_data):
    vectorizer = TfidfVectorizer(max_features=1500, ngram_range=(1, 3), min_df=2)  # Include bigrams and trigrams
    tfidf_matrix = vectorizer.fit_transform(text_data)
    return tfidf_matrix

text_features = extract_text_features(df_videos['cleaned_description'])

# Step 4: Extract Audio Features (Mel-spectrogram)
def extract_audio_features(audio_file):
    y, sr = librosa.load(audio_file, duration=60)  # Load first 60 seconds of audio
    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr)  # Mel-spectrogram
    mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)  # Convert to dB
    return mel_spectrogram_db.mean(axis=1)  # Average over time

audio_features = extract_audio_features('education_audio.mp3')

# Step 5: Extract Video Frames (Optional Visual Features)
def extract_video_frames(video_file, frame_count=5):
    cap = cv2.VideoCapture(video_file)
    frames = []
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    for i in range(frame_count):
        cap.set(1, total_frames // frame_count * i)  # Skip to the next frame
        ret, frame = cap.read()
        if ret:
            frames.append(frame)

    cap.release()
    return frames

video_frames = extract_video_frames('education_video.mp4')

# Step 6: Combine Text, Audio Features
combined_features = np.hstack((text_features.toarray(), np.tile(audio_features.reshape(-1,1).T, (text_features.shape[0], 1)))) # Repeat audio features for each text entry and then hstack

# Step 7: Feature Scaling
scaler = StandardScaler()
scaled_features = scaler.fit_transform(combined_features)

# Step 8: Dimensionality Reduction with PCA
pca = PCA(n_components=0.95, random_state=0)  # Retain 95% variance
reduced_features = pca.fit_transform(scaled_features)

# Step 9: Further Dimensionality Reduction with t-SNE
tsne = TSNE(n_components=2, random_state=0, perplexity=5, init='pca')  # Set perplexity to 5 and init to PCA for stability
tsne_features = tsne.fit_transform(reduced_features)

# Step 10: KMeans Clustering
def cluster_videos(features, num_clusters=2):
    kmeans = KMeans(n_clusters=num_clusters, random_state=0, n_init=20)  # Increase n_init to stabilize clustering
    clusters = kmeans.fit_predict(features)
    return clusters

# Step 11: DBSCAN Clustering
def cluster_with_dbscan(features, eps=0.3, min_samples=2):
    dbscan = DBSCAN(eps=eps, min_samples=min_samples)
    clusters = dbscan.fit_predict(features)
    return clusters

# Step 12: Silhouette Score Evaluation
def evaluate_clustering(features, labels):
    if len(np.unique(labels)) > 1:  # Ensure there are multiple clusters
        score = silhouette_score(features, labels)
    else:
        score = -1  # Invalid score if there's only 1 cluster
    return score

# Step 13: Finding Optimal Number of Clusters (K)
def find_best_k(features, min_k=2, max_k=10):
    best_k = min_k
    best_score = -1
    for k in range(min_k, max_k):
        clusters = KMeans(n_clusters=k, random_state=0, n_init=20).fit_predict(features)
        score = silhouette_score(features, clusters)
        if score > best_score:
            best_k = k
            best_score = score
    return best_k, best_score

# Finding the best K
best_k, best_score = find_best_k(tsne_features, min_k=2, max_k=5)
print(f'Best K: {best_k}, Best Silhouette Score: {best_score}')

# Final Clustering with KMeans
kmeans_clusters = cluster_videos(tsne_features, num_clusters=best_k)

# Optional: Use DBSCAN and check its silhouette score
dbscan_clusters = cluster_with_dbscan(tsne_features, eps=0.3, min_samples=2)
if len(np.unique(dbscan_clusters)) > 1:
    dbscan_silhouette = evaluate_clustering(tsne_features, dbscan_clusters)
    print(f'DBSCAN Silhouette Score: {dbscan_silhouette}')
else:
    print("DBSCAN resulted in a single cluster, silhouette score cannot be calculated.")

# Optional: Agglomerative Clustering
agg_clustering = AgglomerativeClustering(n_clusters=best_k)
agg_clusters = agg_clustering.fit_predict(tsne_features)
agg_silhouette = evaluate_clustering(tsne_features, agg_clusters)
print(f'Agglomerative Clustering Silhouette Score: {agg_silhouette}')

# Assign clusters to DataFrame and display
df_videos['cluster'] = kmeans_clusters
print(df_videos[['title', 'cluster']])

# Final Silhouette Score after clustering
final_silhouette = evaluate_clustering(tsne_features, df_videos['cluster'])
print(f'Final Silhouette Score: {final_silhouette}')

import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

def visualize_clusters(features, clusters):
    pca = PCA(n_components=2)
    reduced_features = pca.fit_transform(features)

    plt.scatter(reduced_features[:, 0], reduced_features[:, 1], c=clusters, cmap='viridis')
    plt.xlabel('Component 1')
    plt.ylabel('Component 2')
    plt.title('Video Clusters')
    plt.show()

# Example usage
# Assuming you want to visualize the KMeans clusters:
visualize_clusters(text_features.toarray(), kmeans_clusters)